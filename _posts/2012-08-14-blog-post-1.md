---
title: 'Evaluating Large Language Models'
date: 2024-08-24
permalink: /posts/2024/08/LLM_eval/
tags:
  - LLM
  - Benchmarking
---
Benchmarking LLMs
======

In this blog, I write about some of my experience with LLM Benchmarking. 

I have worked on different LLM projects covering Retrieval Augmented Generation, Differiential Diagnosis and Triaging. Therefore, they are 3 parts to this blog dedicated to each section. 

The Necessity of Benchmarking LLMs
======

It turns out that benchmarking LLMs for your specific application is something that's really hard to do. Benchmarking Large Language Models (LLMs) is a critical practice within the field of natural language processing, serving as both a cornerstone for assessing model performance and a roadmap for future improvements. Some of the reasons I have encountered that make LLM benchmarking hard to do are:

1. Benchmarks often fail to capture the breadth and complexity of natural language understanding and generation. Benchmarks may focus on narrow tasks or datasets, leading to models that excel in specific areas but lack broader applicability. 

2. Annotating a good dataset is surprisingly hard. It requires domain experts to do so. Imagine hiring a team of busy professionals like doctors, engineers or investment bankers to annotate datasets for your usecase.

3. The dynamic nature of language and its contextual nuances means that static benchmarks can quickly become outdated.

4. Manual checking the LLM output for a small dataset is usually insufficient. 

While creating a fundamental LLM application may be straightforward, the challenge lies in its ongoing maintenance and continuous enhancement. This is where the ideology of Metrics-Driven Development (MDD) plays a crucial role. MDD is a product development approach that relies on data to make well-informed decisions. This approach entails the ongoing monitoring of essential metrics over time, providing valuable insights into an applicationâ€™s performance


Benchmarking Retrieval Augmented Generation
======
The retrieval step typically involves:

Vectorizing the initial input into an embedding, using an embedding model of your choice (eg. OpenAI's text-embedding-3-large model).
Performing a vector search (by using the previously embedded input) on the vector store that contains your vectorized knowledge base, to retrieve the top-K most "similar" vectorized text chunks in your vector store.
Rerank the retrieved nodes. The initial ranking provided by the vector search might not always align perfectly with the specific relevance for your specific use-case.

The generation step, which follows the retrieval step, typically involves:

Constructing a prompt based on the initial input the previous vector-fetched retrieval context.
Providing this prompt to your LLM. This yields the final augmented output.


1. RAGAS
RAGAS is a popular Python library that evaluates for the retrieval segment and generation segment of RAG applications. 

2. Deepeval
deepeval offers three LLM evaluation metrics to evaluate retrievals:

ContextualPrecisionMetric: evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.

ContextualRecallMetric: evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.

ContextualRelevancyMetric: evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.

deepeval offers two LLM evaluation metrics to evaluate generic generations:

AnswerRelevancyMetric: evaluates whether the prompt template in your generator is able to instruct your LLM to output relevant and helpful outputs based on the retrieval_context.
FaithfulnessMetric: evaluates whether the LLM used in your generator can output information that does not hallucinate AND contradict any factual information presented in the retrieval_context.

Benchmarking Differiential Diagnosis
======
 1. Open Medical Benchmarks and its variants
 Example: MedQA, USMLE, Taiwan Medical Examination, India Medical Examination
 2. Natural Language Inference
 3. G-eval
 Allows you to evaluate generation on ANY customized criteria. The general advise is to use a more advanced model as the LLM Judge based on your predefined criteria.



